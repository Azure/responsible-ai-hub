"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[309],{3536:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var i=n(5893),s=n(1151);const a={id:"cs-intro",title:"Overview",sidebar_position:1,slug:"/content-safety-overview"},o="Overview",r={id:"azure-content-safety/cs-intro",title:"Overview",description:"As we build applications that engage with people, it\u2019s vital that the content that is displayed to the end-user is not harmful or offensive.  In this workshop you will learn how to use the prebuilt AI service, Azure Safety Content, in your applications to ensure that the texts or images that are sent to the user or the user enters do not contain data that has violence, self-harm, hate or sexual information.  In addition, based on the demographic of the end-users that interact with the application, developers can control what content is acceptable based on the sensitivity level of the inappropriate content. The services support multiple languages and multiple industries such as gaming, e-commerce, social media, education, etc.",source:"@site/docs/azure-content-safety/intro.md",sourceDirName:"azure-content-safety",slug:"/content-safety-overview",permalink:"/docs/content-safety-overview",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{id:"cs-intro",title:"Overview",sidebar_position:1,slug:"/content-safety-overview"},sidebar:"contentSafetySidebar",next:{title:"Lab# 1: Launch GitHub codespaces project",permalink:"/docs/content-safety-codespaces"}},c={},l=[{value:"\ud83d\udc69\ud83c\udffd\u200d\ud83d\udcbb | Objectives",id:"--objectives",level:2},{value:"\u2705 |Prerequisites:",id:"-prerequisites",level:2}];function h(e){const t={a:"a",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(t.p,{children:"As we build applications that engage with people, it\u2019s vital that the content that is displayed to the end-user is not harmful or offensive.  In this workshop you will learn how to use the prebuilt AI service, Azure Safety Content, in your applications to ensure that the texts or images that are sent to the user or the user enters do not contain data that has violence, self-harm, hate or sexual information.  In addition, based on the demographic of the end-users that interact with the application, developers can control what content is acceptable based on the sensitivity level of the inappropriate content. The services support multiple languages and multiple industries such as gaming, e-commerce, social media, education, etc."}),"\n",(0,i.jsx)(t.p,{children:"There are several challenges in having a staff of moderators to review the content. The limitations are difficult to manually inspect the enormous amount of text or images. Another challenge is that the staff that are evaluating the text are subjective, insistent on what they classify as a violation, or it can be time consuming. Using the Content Safety AI service makes it possible to catch issues faster and more efficiently. This reduces unnecessary manual repetitive tasks and potentially errors. In addition, the API service has built-in profanity detection.\r\nWith the vast use of generative AI, there are user input prompts and dynamic AI-generated responses, Azure Content Safety serves as a useful tool to safeguard content that can be risky or undesirable."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{src:n(3914).Z+"",width:"853",height:"480"})}),"\n",(0,i.jsx)(t.h2,{id:"--objectives",children:"\ud83d\udc69\ud83c\udffd\u200d\ud83d\udcbb | Objectives"}),"\n",(0,i.jsx)(t.p,{children:"At the end of the workshop, you will:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Learn how to detect and flag text that are unsuitable for end-users."}),"\n",(0,i.jsx)(t.li,{children:"Learn how to block images that are inappropriate."}),"\n",(0,i.jsx)(t.li,{children:"Learn how to create applications with a safe and friendly tone."}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"-prerequisites",children:"\u2705 |Prerequisites:"}),"\n",(0,i.jsx)(t.p,{children:"To complete this workshop, you need the following:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Login or Signup for a ",(0,i.jsx)(t.a,{href:"https://azure.microsoft.com/free/",children:"Free Azure account"})]}),"\n",(0,i.jsx)(t.li,{children:"GitHub account with access to GitHub Codespaces."}),"\n",(0,i.jsx)(t.li,{children:"Install Python 3.9 or higher."}),"\n"]})]})}function u(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},3914:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/cs-severity-levels-e2a871468b7403d6b5e7a4f624b12ba7.gif"},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>o});var i=n(7294);const s={},a=i.createContext(s);function o(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);