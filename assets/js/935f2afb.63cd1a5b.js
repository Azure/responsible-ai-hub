"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"dashboardSidebar":[{"type":"link","label":"Responsible AI Dashboard","href":"/responsible-ai-hub/docs/rai-dashboard-ms-learn","docId":"responsible-ai-dashboard-mslearn/rai-ms-learn","unlisted":false}],"contentSafetySidebar":[{"type":"link","label":"Overview","href":"/responsible-ai-hub/docs/content-safety-overview","docId":"azure-content-safety/cs-intro","unlisted":false},{"type":"link","label":"Lab 1 - GitHub Codespaces","href":"/responsible-ai-hub/docs/content-safety-codespaces","docId":"azure-content-safety/lab1-launch-github-codespace/cs-codespaces-lab1\'","unlisted":false},{"type":"link","label":"Lab 2 -  Create Content Safety instance","href":"/responsible-ai-hub/docs/content-safety-resource","docId":"azure-content-safety/lab2-create-content-safety/content-safety-lab2\'","unlisted":false},{"type":"category","label":"Lab 3\ufe0f -  Setup Azure connections","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Lab# 3: Setup Azure connections","href":"/responsible-ai-hub/docs/setup-azure-connections","docId":"azure-content-safety/lab3-setting-azure-connections/cs-setup-azure-conn-lab3\'","unlisted":false}]},{"type":"link","label":"Lab 4 -  Analyze Text","href":"/responsible-ai-hub/docs/content-safety-analyze-text","docId":"azure-content-safety/lab4-analyze-text/cs-analyze-text-lab4\'","unlisted":false},{"type":"link","label":"Lab 5 - Analyze Image","href":"/responsible-ai-hub/docs/content-safety-analyze-image","docId":"azure-content-safety/lab5-analyze-image/cs-analyze-image-lab4\'","unlisted":false},{"type":"link","label":"Lab 6 - OpenAI playground","href":"/responsible-ai-hub/docs/openai-playground","docId":"azure-content-safety/lab6-openai-playground/openai-playground-lab6\'","unlisted":false},{"type":"link","label":"Lab 7 - Analyze OpenAI responses","href":"/responsible-ai-hub/docs/content-safety-analyze-openai","docId":"azure-content-safety/lab7-analyze-openai-response/cs-analyze-openai-lab7\'","unlisted":false}],"promptFlowSidebar":[{"type":"link","label":"Prompt Flow Overview","href":"/responsible-ai-hub/docs/prompt-flow-overview","docId":"azure-prompt-flow/pf-intro","unlisted":false},{"type":"link","label":"Lab 1\ufe0f - Build workshop environment","href":"/responsible-ai-hub/docs/build-workshop-enviroment","docId":"azure-prompt-flow/lab1-build-workshop-env/prompt-flow-lab1","unlisted":false},{"type":"category","label":"Lab 2 -  Run chatbot template","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Lab# 2: Run chatbot template","href":"/responsible-ai-hub/docs/run-chatbot-template","docId":"azure-prompt-flow/lab2-run-chatbot-template/pf-run-chatbot-template-lab2\'","unlisted":false}]},{"type":"link","label":"Lab 3 - Bring your data","href":"/responsible-ai-hub/docs/bring-your-data","docId":"azure-prompt-flow/lab3-bring-your-data/pf-bring-your-data-lab3\'","unlisted":false},{"type":"link","label":"Lab 4 - Chatbot with your data","href":"/responsible-ai-hub/docs/chatbot-with-your-data","docId":"azure-prompt-flow/lab4-chatbot-with-your-data/pf-chatbot-with-your-data-lab4\'","unlisted":false},{"type":"link","label":"Lab 5 - Run custom chatbot","href":"/responsible-ai-hub/docs/run-custom-chatbot","docId":"azure-prompt-flow/lab5-run-custom-chatbot/pf-run-custom-chatbot-lab5\'","unlisted":false},{"type":"category","label":"Lab 6 - Evaluate Chatbot","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Lab# 6: Evaluate chatbot","href":"/responsible-ai-hub/docs/evaluate-chatbot","docId":"azure-prompt-flow/lab6-evaluate chatbot/pf-evaluate-chatbot-lab6\'","unlisted":false}]}]},"docs":{"azure-content-safety/cs-intro":{"id":"azure-content-safety/cs-intro","title":"Overview","description":"As we build applications that engage with people, it\u2019s vital that the content that is displayed to the end-user is not harmful or offensive.  In this workshop you will learn how to use the prebuilt AI service, Azure Safety Content, in your applications to ensure that the texts or images that are sent to the user or the user enters do not contain data that has violence, self-harm, hate or sexual information.  In addition, based on the demographic of the end-users that interact with the application, developers can control what content is acceptable based on the sensitivity level of the inappropriate content. The services support multiple languages and multiple industries such as gaming, e-commerce, social media, education, etc.","sidebar":"contentSafetySidebar"},"azure-content-safety/lab1-launch-github-codespace/cs-codespaces-lab1\'":{"id":"azure-content-safety/lab1-launch-github-codespace/cs-codespaces-lab1\'","title":"Lab# 1: Launch GitHub codespaces project","description":"To expedite running the workspace and having a consistent project environment, we\u2019ll be using GitHub codespaces.","sidebar":"contentSafetySidebar"},"azure-content-safety/lab2-create-content-safety/content-safety-lab2\'":{"id":"azure-content-safety/lab2-create-content-safety/content-safety-lab2\'","title":"Lab# 2: Create Azure Content Safety","description":"To complete this lesson, you would need a Azure OpenAI and Azure Content Safety. If you don\'t have OpenAI in your subscription, you\'ll only be creating an Azure Content Safety resource.","sidebar":"contentSafetySidebar"},"azure-content-safety/lab3-setting-azure-connections/cs-setup-azure-conn-lab3\'":{"id":"azure-content-safety/lab3-setting-azure-connections/cs-setup-azure-conn-lab3\'","title":"Lab# 3: Setup Azure connections","description":"You run this lab you will need to authenticate to Azure and set your Azure subscription.  You will also need to set the Azure Content Safety endpoint and key as environment variables in the notebook.  The following steps will walk you through the process.","sidebar":"contentSafetySidebar"},"azure-content-safety/lab4-analyze-text/cs-analyze-text-lab4\'":{"id":"azure-content-safety/lab4-analyze-text/cs-analyze-text-lab4\'","title":"Lab# 4: Analyze Text","description":"When dealing with text, it is either user or application generated. With Generative AI, we need to be mindful that the text to be examined is either user input prompts or AI generated responses. For example, one of the challenges of using social media or Chat applications to a demographic of high school students is that it introduces system vulnerabilities for users to enter text that have profanity, bullying, harassment, derogatory implications etc.","sidebar":"contentSafetySidebar"},"azure-content-safety/lab5-analyze-image/cs-analyze-image-lab4\'":{"id":"azure-content-safety/lab5-analyze-image/cs-analyze-image-lab4\'","title":"Lab# 5: Analyze Image","description":"There are tons of applications and social medial sites that enable users to upload images. This opens a flood gate of opportunities for users to upload sexual derogative content, violence, or harmful content. Similar to text, it\u2019s not realistic to rely on users to flag inappropriate content or the staff to manually see the content when it\u2019s uploaded.  Even with manual monitors, images are subjected to each individual evaluator to determine if it is risky.","sidebar":"contentSafetySidebar"},"azure-content-safety/lab6-openai-playground/openai-playground-lab6\'":{"id":"azure-content-safety/lab6-openai-playground/openai-playground-lab6\'","title":"Lab# 6: OpenAI Playground","description":"Bring your own laptop - make sure your laptop is fully charged!","sidebar":"contentSafetySidebar"},"azure-content-safety/lab7-analyze-openai-response/cs-analyze-openai-lab7\'":{"id":"azure-content-safety/lab7-analyze-openai-response/cs-analyze-openai-lab7\'","title":"Lab# 7: Analyze OpenAI Response","description":"OpenAI produces dynamic responses where one response to the same inquiry does not alway yeild the same output.  In addition, we cannot always anticipate the risks that are in the response that can could be harmful to users.  Azure Content Safety help you to add content filter and guardrails for categories such as sexual derogative content, violence, or harmful content.","sidebar":"contentSafetySidebar"},"azure-prompt-flow/lab1-build-workshop-env/prompt-flow-lab1":{"id":"azure-prompt-flow/lab1-build-workshop-env/prompt-flow-lab1","title":"Exercise# 1: Build Workshop Environment","description":"As you work on creating Flows, it may have dependencies, services or external resources that you would need to connect to; such as OpenAI, Content Safety AI or your custom LLM models.  It enables users to add and manage connection to these resources as well as a their connection secrets (e.g. name, api key, api_endpoint, or type).","sidebar":"promptFlowSidebar"},"azure-prompt-flow/lab2-run-chatbot-template/pf-run-chatbot-template-lab2\'":{"id":"azure-prompt-flow/lab2-run-chatbot-template/pf-run-chatbot-template-lab2\'","title":"Lab# 2: Run chatbot template","description":"Azure Machine Learning studio promptflot provide a gallery of flows templates to build on.  We will start by using a basic chat template that interacts with prompts powered by an OpenAI model.","sidebar":"promptFlowSidebar"},"azure-prompt-flow/lab3-bring-your-data/pf-bring-your-data-lab3\'":{"id":"azure-prompt-flow/lab3-bring-your-data/pf-bring-your-data-lab3\'","title":"Lab# 3: bring your own data","description":"Open AI and most LLM models are training from various publicly available data.  However, there are instances where we need to use our own data and narrow the actions and data search of our LLM prompts to focus only on the scope of our data or expand the data from LLM model to include our data as well.  To use your own data in a LLM, you need to convert you data into numeric values.  Each word mapping to a specific number (token).  Then you train a model to find similarities, collations, or word association, the model creates vector indexes to the word associations.   The good thing is the Prompt Flow service provide an easy-to-use process your to upload dataset and it generates model and the Vector indexes.","sidebar":"promptFlowSidebar"},"azure-prompt-flow/lab4-chatbot-with-your-data/pf-chatbot-with-your-data-lab4\'":{"id":"azure-prompt-flow/lab4-chatbot-with-your-data/pf-chatbot-with-your-data-lab4\'","title":"Lab# 4: Chatbot to use your data","description":"In the precise exercise you create a vector index and train to search for your vector embeddings.  In the exercise, you\u2019ll be expanding the Chat pipeline logic to take the user question and convert to numeric embeddings.  Then we\u2019ll use the numeric embedding to search the numeric vector.  Next, we\u2019ll use the prompt to set rules with restrictions and how to display the data to the user.","sidebar":"promptFlowSidebar"},"azure-prompt-flow/lab5-run-custom-chatbot/pf-run-custom-chatbot-lab5\'":{"id":"azure-prompt-flow/lab5-run-custom-chatbot/pf-run-custom-chatbot-lab5\'","title":"Lab# 5: Run custom chatbot","description":"Now that you have updated the prompt flow logic to you use your own data and process the output, let\u2019s see if the Chat will generate relevant information pertaining to our Contoso dental data.","sidebar":"promptFlowSidebar"},"azure-prompt-flow/lab6-evaluate chatbot/pf-evaluate-chatbot-lab6\'":{"id":"azure-prompt-flow/lab6-evaluate chatbot/pf-evaluate-chatbot-lab6\'","title":"Lab# 6: Evaluate chatbot","description":"You can unit test your Flow.  However, Prompt flow provides a gallery of sample evaluation flows your can use to test you Flow in bulk. For example, classification accuracy, QnA Groundedness, QnA Relevant, QnA Similarity, QnA F1 Score etc.  This enables you to test how well your LLM is performing.  In addition, you have the ability to examine which of your variant prompts are performing better.  In this example, we\u2019ll use the QnA RAG Evaluation template to test our flow.","sidebar":"promptFlowSidebar"},"azure-prompt-flow/pf-intro":{"id":"azure-prompt-flow/pf-intro","title":"Prompt Flow Overview","description":"Prompt engineering is a tedious process that involves a lot tasks and components.  Developments have next determine what the input or prompts are going to be and what the actions we want in return.  In order to achieve, there are a lot of parts.  For instance, the prompts are responses need to be tokenize.  Next, depending on that the action that will be the output, we need to identify where that information is coming from.  Is the information coming from an API, or an LLM model?  When data is returned, does it need preprocessing?  How is the best response identify?","sidebar":"promptFlowSidebar"},"Cleanup":{"id":"Cleanup","title":"Cleanup","description":"Be sure to shutdown the compute instance at the end of this workshop so you do not run out of credits."},"References":{"id":"References","title":"Tutorials","description":"To get a deeper understanding of all the Responsible AI dashboard features and components, refer to the following tutorials series:"},"responsible-ai-dashboard-mslearn/rai-ms-learn":{"id":"responsible-ai-dashboard-mslearn/rai-ms-learn","title":"Responsible AI Dashboard","description":"In this session, you\'ll complete a hands-on lab that will teach you how to debug an AI model using the Responsible AI Dashboard in Azure Machine Learning Studio, to ensure that  it performs responsibly and is less harmful.","sidebar":"dashboardSidebar"},"Survey":{"id":"Survey","title":"Survey","description":"Congratulations on completing the workshop! We hope you enjoyed the experience and learned a lot about the Responsible AI dashboard. We would love to hear your feedback on the training. Please take a few minutes to complete the survey below."}}}')}}]);